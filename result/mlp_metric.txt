============== Train Info ==============
Dataset number: 3
Model: MLP
Train: 2016-09-01T00:00:00+00:00 --> 2016-11-30T00:00:00+00:00
Val: 2016-12-01T00:00:00+00:00 --> 2016-12-31T00:00:00+00:00
Test: 2017-01-01T00:00:00+00:00 --> 2017-01-31T00:00:00+00:00
City number: 13
Use metero: ['2m_temperature', 'boundary_layer_height', 'k_index', 'relative_humidity+950', 'surface_pressure', 'total_precipitation', 'u_component_of_wind+950', 'v_component_of_wind+950']
batch_size: 32
epochs: 50
hist_len: 8
pred_len: 8
cycle_len: 8
weight_decay: 1e-05
early_stop: 10
lr: 0.001
========================================
MLP(
  (fc_in): Linear(in_features=13, out_features=16, bias=True)
  (fc_out): Linear(in_features=16, out_features=1, bias=True)
  (mlp): Sequential(
    (0): Linear(in_features=16, out_features=16, bias=True)
    (1): Sigmoid()
    (2): Linear(in_features=16, out_features=16, bias=True)
    (3): Sigmoid()
  )
)---------------------------------------
train_loss | mean: 0.3708 std: 0.0043
val_loss   | mean: 0.3779 std: 0.0073
test_loss  | mean: 0.4227 std: 0.0076
RMSE       | mean: 39.3467 std: 0.5916
MAE        | mean: 23.6006 std: 0.4271
CSI        | mean: 0.8659 std: 0.0038
POD        | mean: 0.9686 std: 0.0027
FAR        | mean: 0.1091 std: 0.0053
MAPE        | mean: 0.3842 std: 0.0208

=================== Model Info ===================
model params: 785
GPU memory usage during training: 2.0020GB
GPU memory usage during testing: 0.0215GB
Training time: 112.8595s
Inference time: 0.2301s
Model file path: E:/jupyter/match/create/data/results\8_8\3\MLP\20250331224018\09\model.pth
