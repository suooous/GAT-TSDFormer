============== Train Info ==============
Dataset number: 3
Model: iTransformer
Train: 2016-09-01T00:00:00+00:00 --> 2016-11-30T00:00:00+00:00
Val: 2016-12-01T00:00:00+00:00 --> 2016-12-31T00:00:00+00:00
Test: 2017-01-01T00:00:00+00:00 --> 2017-01-31T00:00:00+00:00
City number: 13
Use metero: ['2m_temperature', 'boundary_layer_height', 'k_index', 'relative_humidity+950', 'surface_pressure', 'total_precipitation', 'u_component_of_wind+950', 'v_component_of_wind+950']
batch_size: 32
epochs: 50
hist_len: 8
pred_len: 8
cycle_len: 8
weight_decay: 1e-05
early_stop: 10
lr: 0.001
========================================
iTransformer(
  (fc_out): Linear(in_features=32, out_features=1, bias=True)
  (gru_cell): GRUCell(
    (x2h): Linear(in_features=14, out_features=96, bias=True)
    (h2h): Linear(in_features=32, out_features=96, bias=True)
  )
  (graph_mlp): Sequential(
    (0): Linear(in_features=169, out_features=13, bias=True)
    (1): Sigmoid()
  )
)---------------------------------------
train_loss | mean: 0.1933 std: 0.0119
val_loss   | mean: 0.3130 std: 0.0064
test_loss  | mean: 0.3971 std: 0.0051
RMSE       | mean: 37.0274 std: 0.2300
MAE        | mean: 22.0629 std: 0.1752
CSI        | mean: 0.8867 std: 0.0024
POD        | mean: 0.9695 std: 0.0031
FAR        | mean: 0.0879 std: 0.0036
MAPE        | mean: 0.3022 std: 0.0095

=================== Model Info ===================
model params: 6851
GPU memory usage during training: 10.3276GB
GPU memory usage during testing: 0.1133GB
Training time: 191.5296s
Inference time: 0.6924s
Model file path: E:/jupyter/match/create/data/results\8_8\3\iTransformer\20250331213058\09\model.pth
